---
title: "Trying out Llama3.1 8B, using R and Ollama"
author: "Andi Fugard ([@andi@sciences.social](https://sciences.social/@andi))"
date: "31 July 2024"
output:
  html_document:
    df_print: paged
  html_notebook:
    code_folding: none
  word_document: default
---


This is a test of the 8 billion parameter version of Llama 3.1, the new LLM from Meta, running locally via [Ollama](https://ollama.com/). I'm going to use it to try to classify articles on Google Scholar by their title and abstract.

Include some packages:

```{r}
#devtools::install_github("hauselin/ollamar")
library(conflicted)
library(ollamar)
library(tictoc)
library(scholar)
library(beepr)
library(tidyverse)
```

Here are the models I currently have loaded on Ollama:

```{r}
list_models()
```

Grab data from Google Scholar (I'm using my own profile). Note the caching here using RDS files, so that I'm not continually re-requesting the same data (get_publications does its own caching but I'm not sure how, so I've switched it off).

```{r}
get_abs <- Vectorize(function(pid) {
  res <- get_publication_abstract(id = "xrY7bFYAAAAJ", pub_id = pid)
  paste(res, collapse = "\n")
})

if (file.exists("scholar_stash.rds")) {
  papers <- readRDS("scholar_stash.rds")
} else {
  papers <- get_publications("xrY7bFYAAAAJ", flush = TRUE) |>
    mutate(abstract = get_abs(pubid))
  saveRDS(papers, "scholar_stash.rds")
}
papers
```


Here's the prompt:

```{r}
title_abstract_prompt <- function(t, a) {
  sprintf(
    "Instructions: I would like you to classify journal articles by academic discipline and subdiscipline please, based only on the article's title and abstract. If you don't know, answer 'NA'. Be concise, using a small number of words. If the article belongs to more than one category, separate each one with '|'. An example response would be 'psychology|reasoning|evaluation'. Another example response would be 'research methods|qualitative'. Your answer should be the category or categories, with no other text, no quotation marks, do not provide an explanation, and all lower case please. Use British English naming conventions. The input is:\n\nTitle: %s\n\nAbstract: %s",
    t,
    a
  )
}
title_abstract_prompt("Example title", "An example abstract") |> cat()
```

BBC's Henry Cooke has written [a neat article](https://www.bbc.co.uk/rd/blog/2024-06-mitigating-llm-hallucinations-in-text-summarisation) on designing good prompts -- I read it after devising the mediocre prompt above by trial and error. Such is life :-)


This function calls Ollama:

```{r}
classify_paper <- Vectorize(function(t, a) {
  generate("llama3.1:8b", title_abstract_prompt(t, a), output = "text") |> as.vector()
})
```


Do it for all papers in the Scholar stash (when I was developing the code, I used this line to select two or three papers before running on all):

```{r}
papers_to_analyse <- papers
```

I'm using beepr to let me know when it's done (note I'm using caching again as this can take 1 to 2 minutes per paper):

```{r}
tic()
if (file.exists("llama_out.rds")) {
  papers_to_analyse <- readRDS("llama_out.rds")
} else {
  papers_to_analyse$res <- classify_paper(papers_to_analyse$title,
                                          papers_to_analyse$abstract)
  saveRDS(papers_to_analyse, "llama_out.rds")
}
toc()
beep(2)
```

Take a look:

```{r}
result <- papers_to_analyse |>
  mutate(
    res = ifelse(res == "na", NA, res)
  )
result |>
  mutate(title = str_trunc(title, 30),
         res   = str_trunc(res, 30)) |>
  select(title, res)
```


Now I want to reshape the data to tidy format:

```{r}
wide_topics_mat <- result$res |> str_split_fixed("\\|", n = Inf)
colnames(wide_topics_mat) <- paste0("t_",1:ncol(wide_topics_mat))
wide_topics <- as_tibble(wide_topics_mat)
wide_topics
```


```{r}
res_topics <- bind_cols(result |> select(title), wide_topics) |>
  pivot_longer(
    cols = starts_with("t_"),
    values_to = "class",
    names_prefix = "t_",
    names_to = "topic_num"
  ) |>
  dplyr::filter(class != "") |>
  mutate(class = as_factor(class))
```


Tidy up the levels a little:

```{r}
levels(res_topics$class) |> sort()
```


```{r}
res_topics_clean <- res_topics |>
  mutate(class = fct_recode(class,
                             "mental health care" = "mhc",
                             "mental health care" = "mhc (mental health care)",
                             "HCI" = "human-computer interaction"))
levels(res_topics_clean$class) |> sort()
```


```{r}
res_topics_clean |>
  mutate(title = str_trunc(title, 30)) |>
  select(title, class)
```


Summarise:

```{r}
res_topics_clean |>
  group_by(class) |>
  tally() |>
  arrange(desc(n))
```


Lob it at a cluster analysis:

```{r fig.height=15, fig.width=10, dpi=300}
res_topics_binary <- res_topics_clean |>
  mutate(val = 1) |>
  dplyr::select(-topic_num) |>
  pivot_wider(names_from = "class",
              values_from = val,
              values_fill = 0)

topics_mat <- res_topics_binary |>
  dplyr::select(-title) |>
  as.matrix()
rownames(topics_mat) <- res_topics_binary$title |> str_trunc(50)

dist_mat <- dist(topics_mat, method = "binary")
hc <- hclust(dist_mat, method = "ward.D")

old_par <- par(mar = c(2, 2, 2, 20))
plot(hc |> as.dendrogram(), horiz = TRUE)
par(old_par)
```


This doesn't quite look the way I would have done it manually, e.g., I would have put "How people interpret conditions" alongside "A process model of..." and "Probabilistic theories of reasoning...". But it's not horrendous and impressive given that all I had to do was come up with a prompt.


