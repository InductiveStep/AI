---
title: "Trying out Llama3.1 8B, using R and Ollama"
author: "Andi Fugard ([@andi@sciences.social](https://sciences.social/@andi))"
date: "30 July 2024"
output:
  html_notebook:
    code_folding: none
  html_document:
    df_print: paged
---


This is a test of the 8 billion parameter version of Llama 3.1, the new LLM from Meta, running locally via [Ollama](https://ollama.com/). I'm going to use it to try to classify articles on Google Scholar by their title and abstract.

Include some packages:

```{r}
#devtools::install_github("hauselin/ollamar")
library(conflicted)
library(ollamar)
library(tictoc)
library(scholar)
library(tidyverse)
library(beepr)
```

Here are the models I currently have loaded on Ollama:

```{r}
list_models()
```

Grab data from Google Scholar (I'm using my own profile):

```{r}
get_abs <- Vectorize(function(pid) {
  res <- get_publication_abstract(id = "xrY7bFYAAAAJ", pub_id = pid)
  paste(res, collapse = "\n")
})

papers <- get_publications("xrY7bFYAAAAJ") |>
  mutate(abstract = get_abs(pubid))
papers
```


Here's an example to test out the prompt:

```{r}
example_title    <- "ChatGPT is bullshit"
example_abstract <- "Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called “AI hallucinations”. We argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. We distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. We further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems."
```


```{r}
title_abstract_prompt <- function(t, a) {
  sprintf(
    "Instructions: I would like you to classify journal articles by academic discipline and subdiscipline please, based only on the article's title and abstract. If you don't know, answer 'NA'. Be concise, using a small number of words. If the article belongs to more than one category, separate each one with '|'. An example response would be 'psychology|reasoning|evaluation'. Another example response would be 'research methods|qualitative'. Your answer should be the category or categories, with no other text, no quotation marks, do not provide an explanation, and all lower case please. Use British English naming conventions. The input is:\n\nTitle: %s\n\nAbstract: %s",
    t,
    a
  )
}
title_abstract_prompt(example_title, example_abstract) |> cat()
```

BBC's Henry Cooke has written [a neat article](https://www.bbc.co.uk/rd/blog/2024-06-mitigating-llm-hallucinations-in-text-summarisation) on designing good prompts -- I read it after devising the mediocre prompt above by trial and error. Such is life :-)


This function calls Ollama:

```{r}
classify_paper <- Vectorize(function(t, a) {
  generate("llama3.1:8b", title_abstract_prompt(t, a), output = "text") |> as.vector()
})
```


Do it for all papers in the Scholar stash:

```{r}
papers_to_analyse <- papers
```

I'm using beepr to let me know when it's done:

```{r}
tic()
papers_to_analyse$res <- classify_paper(papers_to_analyse$title,
                                        papers_to_analyse$abstract)
toc()
beep(2)
```

Take a look:

```{r}
result <- papers_to_analyse |>
  mutate(
    res = ifelse(res == "na", NA, res)
  )
result |>
  mutate(title = str_trunc(title, 15),
         res   = str_trunc(res, 45)) |>
  select(title, res)
```


Now I want to reshape the data to tidy format:

```{r}
wide_topics_mat <- result$res |> str_split_fixed("\\|", n = Inf)
colnames(wide_topics_mat) <- paste0("t_",1:ncol(wide_topics_mat))
wide_topics <- as_tibble(wide_topics_mat)
wide_topics
```


```{r}
res_topics <- bind_cols(result |> select(title), wide_topics) |>
  pivot_longer(
    cols = starts_with("t_"),
    values_to = "class",
    names_prefix = "t_",
    names_to = "topic_num"
  ) |>
  dplyr::filter(class != "") |>
  mutate(class = as_factor(class))
```


Tidy up the levels a little:

```{r}
levels(res_topics$class) |> sort()
```


```{r}
res_topics_clean <- res_topics |>
  mutate(class = fct_recode(class,
                             "mental health care" = "mhc",
                             "mental health care" = "mhc (mental health care)",
                             "HCI" = "human-computer interaction"))
levels(res_topics_clean$class) |> sort()
```


```{r}
res_topics_clean |>
  mutate(title = str_trunc(title, 30)) |>
  select(title, class)
```


Summarise:

```{r}
res_topics_clean |>
  group_by(class) |>
  tally() |>
  arrange(desc(n))
```


Lob it at a cluster analysis:

```{r}
res_topics_binary <- res_topics_clean |>
  mutate(val = 1) |>
  dplyr::select(-topic_num) |>
  pivot_wider(names_from = "class",
              values_from = val,
              values_fill = 0)

topics_mat <- res_topics_binary |>
  dplyr::select(-title) |>
  as.matrix()
rownames(topics_mat) <- res_topics_binary$title |> str_trunc(20)
```


```{r fig.height=10, fig.width=10}
dist_mat <- dist(topics_mat, method = "binary")
hc <- hclust(dist_mat)
plot(hc)
```



